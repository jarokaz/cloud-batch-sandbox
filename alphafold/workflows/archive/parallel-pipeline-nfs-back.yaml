# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# [START workflows_batch_primegen]
main:
  params: [args]
  steps:
    - init:
        assign:
          - projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - imageUri: ${"gcr.io/" + projectId + "/alphafold"}
          - jobId: ${"job-alphafold-" + string(int(sys.now()))}
          - jobPath: ${args.stagingLocation + "/" + jobId + "/"}
          - sequencePath: ${jobPath + "sequence.fasta"}
          - nfsConfig:
              ip_address: ${args.nfs_ip_address}
              nfs_path: ${args.nfs_path}
          - networkConfig:
              network: ${"projects/" + projectId + "/global/networks/" + args.network}
              subnetwork: ${"projects/" + projectId + "/regions/" + args.region + "/subnetworks/" + args.subnet}
          - supportedMachineConfigs: {}
          - supportedMachineConfigs["c2-standard-16"]:
                machineType: "c2-standard-16"
                cpuMilli: 16000
                memoryMib: 64000
                bootDiskMib: 200000
          - supportedMachineConfigs["c2-standard-30"]:
                machineType: "c2-standard-30"
                cpuMilli: 30000
                memoryMib: 120000
                bootDiskMib: 200000
          - supportedMachineConfigs["n1-standard-8-t4"]:
                machineType: "n1-standard-8"
                cpuMilli: 8000
                memoryMib: 30000
                bootDiskMib: 200000
                gpuType: "nvidia-tesla-t4"
                gpuCount: 1
          - supportedMachineConfigs["a2-highgpu-1g"]:
                machineType: "a2-highgpu-1g"
                cpuMilli: 12000
                memoryMib: 85000
                bootDiskMib: 200000
                gpuType: "nvidia-tesla-a100"
                gpuCount: 1
          - dataPipelineMachineConfig: ${default(map.get(supportedMachineConfigs, args.dataPipelineMachineType), supportedMachineConfigs["c2-standard-16"])}
          - predictRelaxMachineConfig: ${default(map.get(supportedMachineConfigs, args.predictRelaxMachineType), supportedMachineConfigs["n1-standard-8-t4"])}
          - runRecord:
              ModelPreset: 
                stringValue: ${args.modelPreset}
              Sequence: 
                stringValue: ${args.fastaSequence}
              DbPreset: 
                stringValue: ${args.dbPreset}
              jobPath: 
                stringValue: ${jobPath}
              numMultimerPredictionsPerModel:
                integerValue: ${args.numMultimerPredictionsPerModel}
              runRelax: 
                booleanValue: ${args.runRelax}
    - copySequence:
        call: googleapis.storage.v1.objects.copy
        args:
            destinationBucket: ${text.url_encode(text.split(sequencePath, "/")[0])} 
            destinationObject: ${text.url_encode(text.substring(sequencePath, len(text.split(sequencePath, "/")[0]) + 1, len(sequencePath)))}
            sourceBucket: ${text.url_encode(text.split(args.fastaSequence, "/")[0])}
            sourceObject: ${text.url_encode(text.substring(args.fastaSequence, len(text.split(args.fastaSequence, "/")[0]) + 1, len(args.fastaSequence)))}

    - startRunLog:
        call: googleapis.firestore.v1.projects.databases.documents.createDocument
        args:
          collectionId: ${args.runsCollection}
          parent: ${"projects/"+projectId+"/databases/(default)/documents"}
          documentId: ${jobId}
          body:
            fields: ${runRecord}
        result: runRecordDocument
    
    - callDataPipeline:
        call: runDataPipeline
        args:
          fastaSequence: ${sequencePath}
          maxTemplateDate: ${args.maxTemplateDate}
          jobId: ${jobId}
          jobPath: ${jobPath}
          nfsConfig: ${nfsConfig}
          networkConfig: ${networkConfig}
          modelPreset: ${args.modelPreset}
          dbPreset: ${args.dbPreset}
          imageUri: ${imageUri}
          region: ${args.region}
          projectId: ${projectId}
          runRecordName: ${runRecordDocument.name}
          machineConfig: ${dataPipelineMachineConfig}
        result: dataPipelineOutput

    - callPredictRelax:
        call: runPredictRelax
        args:
          jobPath: ${jobPath}
          inputFeaturesPath: ${dataPipelineOutput.featuresFilePath}
          jobId: ${jobId}
          modelParamsPath: ${args.modelParamsPath} 
          modelPreset: ${args.modelPreset} 
          runRelax: ${args.runRelax} 
          imageUri: ${imageUri}
          region: ${args.region}
          projectId: ${projectId}
          runRecordName: ${runRecordDocument.name}
          numMultimerPredictionsPerModel: ${args.numMultimerPredictionsPerModel}
          randomSeed: ${args.randomSeed}
          parallelism: ${args.parallelism}
          machineConfig: ${predictRelaxMachineConfig} 
        result: predictRelaxResult
        


runDataPipeline:
  params: [jobId, fastaSequence, jobPath, modelPreset, dbPreset, maxTemplateDate, nfsConfig, networkConfig, imageUri, projectId, region, runRecordName, machineConfig]
  steps:
    - init:
        assign:
          - batchJobId: ${jobId + "-data-pipeline"}
          - batchApi: "batch.googleapis.com/v1"
          - batchApiUrl: ${"https://" + batchApi + "/projects/" + projectId + "/locations/" + region + "/jobs"}
          - maxRetryCount: 2
          - maxRunDuration: "7200s"
          - metadataFileName: "data_pipeline.json"
          - msasFolder: "msas"
          - featuresFileName: "features.pkl"
          - sequenceBucket: ${text.split(fastaSequence, "/")[0]}
          - sequenceObject: ${text.substring(fastaSequence, len(text.split(fastaSequence, "/")[0]) + 1, len(fastaSequence))}
          - stepRecord:
              featuresPath: 
                stringValue: ${jobPath + featuresFileName}
              metadataPath: 
                stringValue: ${jobPath + metadataFileName}
              msasPath:
                stringValue: ${jobPath + msasFolder}
          - refDbsMountPath: "/mnt/disks/ref_dbs"
          - jobDirMountPath: "/mnt/disks/job_dir"
          - inputDirMountPath: "/mnt/disks/input"

    - updateRunRecord:
        call: googleapis.firestore.v1.projects.databases.documents.patch
        args:
          name: ${runRecordName + "/steps/data_pipeline"}
          body:
            fields: ${stepRecord}
    #- mockReturn:
    #    return:
    #      featuresFilePath: "jk-aff-bucket/batch-jobs/job-alphafold-1668218762/features.pkl"
    #      metadataFilePath: "jk-aff-bucket/batch-jobs/job-alphafold-1668218762/metadata.json"

    - runDataPipelineCloudBatchJob:
        call: http.post
        args:
          url: ${batchApiUrl}
          query:
            job_id: ${batchJobId}
          headers:
            Content-Type: application/json
          auth:
            type: OAuth2
          body:
            taskGroups:
              taskSpec:
                runnables:
                  - container:
                      imageUri: ${imageUri}
                      entrypoint: "python"
                      commands:
                        - "/runners/run_data_pipeline.py"
                    environment:
                      variables:
                        FASTA_INPUT_PATH: ${inputDirMountPath + "/" + sequenceObject}
                        MSAS_OUTPUT_PATH: ${jobDirMountPath + "/" + msasFolder}
                        FEATURES_OUTPUT_PATH: ${jobDirMountPath + "/" + featuresFileName}
                        METADATA_OUTPUT_PATH: ${jobDirMountPath + "/" + metadataFileName}
                        MODEL_PRESET: ${modelPreset}
                        DB_PRESET: ${dbPreset}
                        MAX_TEMPLATE_DATE: ${maxTemplateDate}
                        REF_DBS_ROOT_PATH: ${refDbsMountPath}
                volumes:
                  - nfs: 
                      server: ${nfsConfig.ip_address}
                      remotePath: ${nfsConfig.nfs_path}
                    mountPath: ${refDbsMountPath}
                  - gcs:
                      remotePath: ${jobPath}
                    mountPath: ${jobDirMountPath}
                  - gcs:
                      remotePath: ${sequenceBucket}
                    mountPath: ${inputDirMountPath}
                computeResource:
                  cpuMilli: ${machineConfig.cpuMilli} 
                  memoryMib: ${machineConfig.memoryMib}
                  bootDiskMib: ${machineConfig.bootDiskMib}
                maxRetryCount: ${maxRetryCount}
                maxRunDuration: ${maxRunDuration}
              taskCount: 1
              parallelism: 1
            allocationPolicy:
              instances:
                - policy:
                    machineType: ${machineConfig.machineType}
              network:
                networkInterfaces:
                  - network: ${networkConfig.network}
                    subnetwork: ${networkConfig.subnetwork}
            logsPolicy:
              destination: CLOUD_LOGGING
        result: createAndRunBatchJobResponse
    - getJob:
        call: http.get
        args:
          url: ${batchApiUrl + "/" + batchJobId}
          auth:
            type: OAuth2
        result: getJobResult
    - logState:
        call: sys.log
        args:
          data: ${"Current job state " + getJobResult.body.status.state}
    - checkState:
        switch:
          - condition: ${getJobResult.body.status.state == "SUCCEEDED"}
            next: returnResult 
          - condition: ${getJobResult.body.status.state == "FAILED"}
            next: failExecution 
        next: sleep
    - sleep:
        call: sys.sleep
        args:
          seconds: 10
        next: getJob
    - returnResult:
        return:
          featuresFilePath: ${jobPath + featuresFileName}
          metadataFilePath: ${jobPath + metadataFileName}
    - failExecution:
        raise:
          message: ${"The underlying batch job " + batchJobId + " failed"}


runPredictRelax:
  params: [jobId, inputFeaturesPath, modelParamsPath,  jobPath,  modelPreset, runRelax, imageUri, projectId, region, runRecordName, numMultimerPredictionsPerModel, randomSeed, parallelism, machineConfig]
  steps:
    - init:
        assign:
          - batchJobId: ${jobId + "-predict-relax"}
          - batchApi: "batch.googleapis.com/v1"
          - batchApiUrl: ${"https://" + batchApi + "/projects/" + projectId + "/locations/" + region + "/jobs"}
          - maxRetryCount: 2
          - maxRunDuration: "7200s"
          - featuresBucket: ${text.split(inputFeaturesPath, "/")[0]}
          - featuresObject: ${text.substring(inputFeaturesPath, len(text.split(inputFeaturesPath, "/")[0]) + 1, len(inputFeaturesPath))}
          - stepRecord:
              rawPredictionsPath: 
                stringValue: ${jobPath}
              unrelaxedProteinsPath: 
                stringValue: ${jobPath}
              relaxedProteinsPath: 
                stringValue: ${jobPath}
              modelParamsPath:
                stringValue: ${modelParamsPath}
              inputFeaturesPath:
                stringValue: ${inputFeaturesPath}
          - taskEnvirons: []
          - numModels: 5
          - numPredictionsPerModel: ${if(modelPreset=="multimer", numMultimerPredictionsPerModel, 1)}
          - jobDirMountPath: "/mnt/disks/job_dir"
          - featuresDirMountPath: "/mnt/disks/features"
          - modelParamsMountPath: "/mnt/disks/params"
    - initPredictionRunners:
        for:
          value: runnerIndex
          range: ${[0, numModels * numPredictionsPerModel - 1]}
          steps:
            - updateTaskEnviron:
                assign:
                  - modelIndex: ${string(runnerIndex // numPredictionsPerModel)}
                  - predictionIndex: ${string(runnerIndex % numPredictionsPerModel)}
                  - taskEnviron: 
                      variables:
                        MODEL_INDEX: ${modelIndex}
                        PRED_INDEX: ${predictionIndex}
                        RANDOM_SEED: ${string(randomSeed + runnerIndex)}
                        MODEL_PARAMS_PATH: ${modelParamsMountPath}
                        INPUT_FEATURES_PATH: ${featuresDirMountPath + "/" + featuresObject}
                        RAW_PREDICTION_PATH: ${jobDirMountPath +  "/result_model_" + modelIndex + "_pred_" + predictionIndex + ".pkl" }
                        UNRELAXED_PROTEIN_PATH: ${jobDirMountPath +  "/unrelaxed_model_" + modelIndex + "_pred_" + predictionIndex + ".pdb"}
                        RELAXED_PROTEIN_PATH: ${jobDirMountPath +  "/relaxed_model_" + modelIndex + "_pred_" + predictionIndex + ".pdb"}
                        PREDICTION_METADATA_PATH: ${jobDirMountPath + "/pred_metadata_model_" + modelIndex + "_pred_" + predictionIndex + ".json"}
                        RELAXATION_METADATA_PATH: ${jobDirMountPath + "/relax_metadata_model_" + modelIndex + "_pred_" + predictionIndex + ".json"}
                        MODEL_PRESET: ${modelPreset}
                        RUN_RELAXATION: ${if(runRelax, "true", "false")}
                        
                  - taskEnvirons: ${list.concat(taskEnvirons, taskEnviron)}

    - updateRunRecord:
        call: googleapis.firestore.v1.projects.databases.documents.patch
        args:
          name: ${runRecordName + "/steps/data_pipeline"}
          body:
            fields: ${stepRecord}
    - runPredictRelaxCloudBatchJob: 
        call: http.post
        args:
          url: ${batchApiUrl}
          query:
            job_id: ${batchJobId}
          headers:
            Content-Type: application/json
          auth:
            type: OAuth2
          body:
            taskGroups:
              taskSpec:
                runnables:
                  - container:
                      imageUri: ${imageUri}
                      entrypoint: "python"
                      commands:
                        - "/runners/run_predict.py"
                      options: "--privileged"
                      volumes:
                        - "/var/lib/nvidia/lib64:/usr/local/nvidia/lib64"
                        - "/var/lib/nvidia/bin:/usr/local/nvidia/bin"
                        - ${modelParamsMountPath + ":" + modelParamsMountPath}
                        - ${jobDirMountPath + ":" + jobDirMountPath}
                        - ${featuresDirMountPath + ":" + featuresDirMountPath}
                  - container:
                      imageUri: ${imageUri}
                      entrypoint: "python"
                      commands:
                        - "/runners/run_relax.py"
                      options: "--privileged"
                      volumes:
                        - "/var/lib/nvidia/lib64:/usr/local/nvidia/lib64"
                        - "/var/lib/nvidia/bin:/usr/local/nvidia/bin"
                        - ${jobDirMountPath + ":" + jobDirMountPath}
                volumes:
                  - gcs: 
                      remotePath: ${modelParamsPath}
                    mountPath: ${modelParamsMountPath}
                  - gcs:
                      remotePath: ${jobPath}
                    mountPath: ${jobDirMountPath}
                  - gcs:
                      remotePath: ${featuresBucket}
                    mountPath: ${featuresDirMountPath}
                computeResource:
                  cpuMilli: ${machineConfig.cpuMilli} 
                  memoryMib: ${machineConfig.memoryMib}
                  bootDiskMib: ${machineConfig.bootDiskMib}
                maxRetryCount: ${maxRetryCount}
                maxRunDuration: ${maxRunDuration}
              taskCount: ${numModels * numPredictionsPerModel}
              taskEnvironments: ${taskEnvirons}
              parallelism: ${parallelism}
            allocationPolicy:
              instances:
                - policy:
                    machineType: ${machineConfig.machineType}
                    accelerators:
                      - type: ${machineConfig.gpuType}
                        count: ${machineConfig.gpuCount}
                  installGpuDrivers: true

            logsPolicy:
              destination: CLOUD_LOGGING
        result: createAndRunBatchJobResponse
    - getJob:
        call: http.get
        args:
          url: ${batchApiUrl + "/" + batchJobId}
          auth:
            type: OAuth2
        result: getJobResult
    - logState:
        call: sys.log
        args:
          data: ${"Current job state " + getJobResult.body.status.state}
    - checkState:
        switch:
          - condition: ${getJobResult.body.status.state == "SUCCEEDED"}
            next: returnResult
          - condition: ${getJobResult.body.status.state == "FAILED"}
            next: failExecution 
        next: sleep
    - sleep:
        call: sys.sleep
        args:
          seconds: 10
        next: getJob
    - returnResult:
        return:
          jobPath: ${jobPath}
    - failExecution:
        raise:
          message: ${"The underlying batch job " + batchJobId + " failed"}